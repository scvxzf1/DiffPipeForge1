{
    "app": {
        "title": "DiffPipe Forge",
        "subtitle": "Made by Tiandong",
        "ready": "System Ready",
        "not_ready": "System Not Ready",
        "python_not_found": "Python Not Found",
        "internal": "native",
        "project_name": "Project Name",
        "monitor_inactive": "Training monitor not yet active.",
        "monitor": {
            "failed_start": "Failed to start TensorBoard",
            "failed_stop": "Failed to stop TensorBoard"
        },
        "select_version": "Select Version",
        "version_win": "Windows Version",
        "version_linux": "Linux Version"
    },
    "common": {
        "reset_default": "Restore Default",
        "confirm_reset_title": "Restore Defaults?",
        "confirm_reset_desc": "Are you sure you want to restore all settings to their default values? This action cannot be undone.",
        "delete_project_title": "Delete Project?",
        "delete_project_desc": "This will permanently delete the project folder,models and all its files. This action cannot be undone!",
        "confirm_delete": "Delete Forever",
        "confirm": "Confirm",
        "cancel": "Cancel",
        "auto_corrected_to_zero": "Negative value detected, auto-corrected to 0",
        "config_saved": "Configuration saved",
        "config_loaded": "Project configuration loaded",
        "project_deleted": "Project deleted",
        "config_not_found": "No valid configuration files found (trainconfig.toml, dataset.toml)",
        "optional": "(Optional)",
        "none": "None",
        "language": "Language",
        "file_copied": "Loaded configuration file: {{name}}",
        "remove_recent": "Remove from history",
        "click_to_rename": "Click to Rename",
        "project_renamed": "Project Renamed Successful",
        "python_switched": "Python environment switched successfully",
        "select_other_path": "Select Other Path...",
        "export": "Export",
        "clear": "Clear",
        "open_folder": "Open Folder",
        "about": "About",
        "browse": "Browse",
        "start": "Start",
        "stop": "Stop",
        "ready": "Ready",
        "running": "Running",
        "coming_soon": "Coming Soon",
        "close": "Close",
        "disabled": "disabled",
        "enabled": "enabled",
        "preview_title": "Directory Preview",
        "no_images_found": "No supported images found in this directory",
        "load_more": "Load More",
        "restore_selected": "Restore",
        "stop_download": "Stop Download",
        "shift_click_hint": "Tip: Shift + Left Click to select range"
    },
    "about": {
        "title": "About App",
        "author": "Author",
        "version": "Version",
        "donated_msg": "This tool is built on the powerful open-source project [Diffusion Pipe](https://github.com/tdrussell/diffusion-pipe). What I did was give it everything I had, trying to wrap it in a beautiful and user-friendly shell.\n\nIf this 'shell' makes your life easier and saves you from wrestling with code, feel free to buy me a coffee.\n\nYour support will help me keep improving, and pays tribute to all open-source contributors.",
        "wechat": "WeChat",
        "alipay": "Alipay"
    },
    "nav": {
        "dataset": "Dataset Config",
        "eval_dataset": "Evaluation Set",
        "training_setup": "Training Setup",
        "training_run": "Run Training",
        "monitor": "Global Monitor",
        "resource_monitor": "Resource Monitor",
        "training_log": "Training Log",
        "toolbox": "Toolbox",
        "main_menu": "Main Menu",
        "system_diagnostics": "Diagnostics"
    },
    "toolbox": {
        "title": "Toolbox",
        "description": "Integrated utilities to improve dataset processing and training efficiency",
        "status_logs": "Task Output Logs",
        "categories": {
            "tagging": "Smart Tagging",
            "quality": "Quality Filter",
            "dedup": "Deduplication",
            "rename": "Rename Tool",
            "aspect": "Aspect Ratio",
            "video": "Video Processing",
            "convert": "Format Converter",
            "filter": "Style Filter"
        },
        "tagging": {
            "image_dir": "Image Directory",
            "api_keys": "API Keys (One per line)",
            "api_keys_hint": "Scripts will use all keys in parallel",
            "api_type": "API Type",
            "api_type_gemini": "Google Gemini",
            "api_type_openai": "OpenAI Compatible",
            "base_url": "Base URL",
            "model_name": "Model Name",
            "prompt": "Prompt",
            "concurrency": "Concurrency per Key",
            "concurrency_hint": "Total Concurrency = API Keys count × Concurrency per Key",
            "started": "Tagging task started",
            "stopped": "Tagging task stopped",
            "finished": "Tagging task finished",
            "no_logs": "Waiting for output...",
            "status_tagged": "TAGGED",
            "status_untagged": "UNTAGGED"
        },
        "quality": {
            "title": "Quality Filter",
            "min_width": "Min Width",
            "min_height": "Min Height",
            "min_size": "Min Size (KB)",
            "blur_threshold": "Blur Threshold (0=Off)",
            "blur_hint": "Rec: 100-300, lower = blurry",
            "noise_threshold": "Noise Threshold (0=Off)",
            "noise_hint": "Rec: 20-50, higher = noisy",
            "min_contrast": "Min Contrast (0=Off)",
            "min_contrast_hint": "Rec: 10-20, lower = flat",
            "threads": "Detection Threads",
            "started": "Quality check started",
            "stopped": "Quality check stopped",
            "finished": "Quality check finished",
            "config": "Configuration"
        },
        "deduplicate": {
            "title": "Deduplication",
            "mode": "Mode",
            "exact": "Exact Match (MD5)",
            "similar": "Visual Similar (pHash)",
            "threshold": "Similarity Threshold",
            "threshold_hint": "Lower is stricter, default 5",
            "started": "Deduplication started",
            "stopped": "Deduplication stopped",
            "finished": "Deduplication finished"
        },
        "rename": {
            "title": "Sequential Rename",
            "prefix": "Prefix",
            "start": "Start Number",
            "ext": "Extension",
            "all": "All Files",
            "png": "PNG Images",
            "webp": "Webp Images",
            "started": "Rename task started",
            "stopped": "Rename task stopped",
            "finished": "Rename task finished"
        },
        "aspect": {
            "title": "Aspect Ratio Stats",
            "target_type": "Target Type",
            "type_image": "Images",
            "type_video": "Videos",
            "mode": "Stats Mode",
            "mode_preset": "Preset (Common Ratios)",
            "mode_custom": "Custom Buckets",
            "bucket_size": "Bucket Size (Step)",
            "bucket_size_hint": "e.g., 0.1 for buckets every 0.1 ratio",
            "started": "Aspect ratio task started",
            "stopped": "Aspect ratio task stopped",
            "finished": "Aspect ratio task finished",
            "visual_analysis": "Visual Analysis",
            "total_count": "Total",
            "avg_ratio": "Average",
            "min_ratio": "Min Ratio",
            "max_ratio": "Max Ratio",
            "analyzing": "Analyzing dataset...",
            "click_to_start": "Start analysis to see results",
            "error_warning": "Warning: {{count}} files could not be processed."
        },
        "errors": {
            "no_dir": "Please select an image directory",
            "no_video_dir": "Please select a video directory",
            "no_keys": "Please enter at least one API key"
        },
        "open_folder": "Open Target Folder",
        "open": "Open Folder",
        "result_preview": "Result Preview",
        "low_quality_preview": "Filtered Results (low_quality)",
        "style_mismatch_preview": "Filtered Results (style_mismatch)",
        "duplicates_preview": "Duplicates (duplicates)",
        "restore_selected": "Restore ({{count}})",
        "restore_success": "Restored {{count}} files",
        "restore_failed": "Restore failed: {{error}}",
        "reset_defaults": "Reset Defaults",
        "confirm_reset": "Confirm Reset?",
        "confirm_reset_desc": "This will clear all current settings and restore them to the initial recommended values.",
        "video": {
            "title": "Video Frame Processing",
            "scan_mode": "Scan Information",
            "convert_mode": "Reduce FPS",
            "target_fps": "Target FPS",
            "fps_hint": "Higher FPS means smoother video but slower processing",
            "visual_analysis": "Video Content Analysis",
            "video_count": "Videos",
            "total_frames": "Total Frames",
            "total_duration": "Total Duration",
            "frame_dist": "Frame Distribution",
            "started": "Video processing started",
            "finished": "Video processing finished",
            "stopped": "Video processing stopped"
        },
        "convert": {
            "title": "Image Converter",
            "config": "Conversion Settings",
            "target_format": "Target Format",
            "threads": "Threads",
            "threads_hint": "Recommended: 2x CPU cores",
            "delete_source": "Delete Source Files",
            "delete_hint": "Source files will be permanently deleted after success!",
            "started": "Conversion started",
            "finished": "Conversion finished",
            "stopped": "Conversion stopped"
        },
        "style_filter": {
            "title": "Style Filter",
            "keep": "Keep Styles",
            "keep_hint": "Images matching these will be kept",
            "remove": "Remove Styles",
            "remove_hint": "Images matching these will be moved to style_mismatch",
            "batch_size": "Batch Size",
            "batch_hint": "Higher for more VRAM (e.g. 128 for 4090)",
            "model": "CLIP Model Path/Name",
            "model_hint": "HuggingFace ID or local model folder path",
            "threads": "Load Threads",
            "threads_hint": "Threads for image loading, recommended 8-16",
            "started": "Style filter started",
            "finished": "Style filter finished",
            "stopped": "Style filter stopped",
            "download_model": "Download Model",
            "download_confirm": "Are you sure you want to start downloading the CLIP model?\n\n- In Chinese environments, it will download via ModelScope.\n- In English environments, it will download via HuggingFace.\n- Please ensure you have a stable internet connection.",
            "no_model": "Local model not detected. Please click 'Download Model' first.",
            "downloading": "Downloading CLIP model...",
            "download_success": "CLIP model downloaded successfully!",
            "download_failed": "Failed to download CLIP model."
        }
    },
    "monitor": {
        "title": "TensorBoard Monitor",
        "initializing": "Initializing Monitor Service...",
        "desc": "Visualize training metrics and images",
        "resource_desc": "Real-time monitoring of CPU, RAM, and GPU usage.",
        "host": "Host",
        "port": "Port",
        "log_dir": "Log Directory",
        "log_dir_placeholder": "Enter training project path or dated folder in output",
        "start": "Start Monitor",
        "stop": "Stop Process",
        "stop_download": "Stop Download",
        "open_browser": "Open in Browser",
        "running": "Running",
        "started": "Monitor started",
        "stopped": "Monitor stopped",
        "not_running_hint": "Click 'Start Monitor' above to view training metrics",
        "cpu_usage": "CPU Usage",
        "system_memory": "System Memory",
        "gpu_core": "GPU Core Load",
        "gpu_memory": "VRAM Usage",
        "no_gpu": "No GPU detected or driver not installed",
        "open_wandb": "Open WandB",
        "wandb_hint_1": "WandB metrics are hosted in the cloud.",
        "wandb_hint_2": "Ensure you enabled WandB in Training Config, then click below to view.",
        "failed_start": "Failed to start TensorBoard",
        "failed_stop": "Failed to stop TensorBoard"
    },
    "theme": {
        "label": "Theme",
        "light": "Light",
        "dark": "Dark"
    },
    "dataset": {
        "title": "Dataset Configuration",
        "eval_title": "Evaluation Dataset Configuration",
        "desc": "Configure your training dataset source and resolution.",
        "eval_desc": "Configure the validation dataset for evaluating model performance.",
        "input_path": "Input Path",
        "control_path": "Control/Context Path",
        "add_path": "Add Path",
        "resolutions": "Resolutions",
        "enable_ar_bucket": "Enable AR Buckets",
        "min_ar": "Min Aspect Ratio",
        "max_ar": "Max Aspect Ratio",
        "num_ar_buckets": "AR Buckets Count",
        "num_repeats": "Repeats (Epochs mult)",
        "advanced_buckets": "Advanced Bucket Overrides",
        "custom_ar_buckets": "Custom AR Buckets",
        "frame_buckets": "Frame Buckets (Video)",
        "save": "Save Configuration",
        "eval_save": "Save Eval Config",
        "enabled": "Enabled",
        "disabled": "Disabled",
        "eval_set_name": "Set Name (for logs)",
        "add_eval_set": "Add Extra Validation Set+",
        "remove_eval_set": "Remove Validation Set",
        "validation_enabled": "Validation Enabled",
        "validation_disabled": "Validation Disabled (Skipped)",
        "validation_enabled_desc": "Model quality will be evaluated during training",
        "validation_disabled_desc": "Click to enable validation steps",
        "input_path_placeholder": "C:\\path\\to\\your\\dataset",
        "validation_path_placeholder": "C:\\path\\to\\validation_data",
        "control_path_placeholder": "C:\\path\\to\\your\\control_images",
        "resolutions_placeholder": "[512] or [[1280, 720]]",
        "custom_ar_buckets_placeholder": "[[512, 512], [448, 576]] or [1.0, 1.5]",
        "frame_buckets_placeholder": "[1, 33, 65]",
        "validation_set": "Validation Set",
        "disable_validation": "Disable Model Validation"
    },
    "model": {
        "title": "Model Configuration",
        "desc": "Select and configure the base model for training.",
        "architecture": "Model Architecture",
        "checkpoint_path": "Checkpoint Path",
        "unet_lr": "UNet LR",
        "text_encoder_1_lr": "Text Encoder 1 LR",
        "text_encoder_2_lr": "Text Encoder 2 LR",
        "diffusers_path": "Diffusers Path",
        "transformer_path": "Transformer Path",
        "flux_shift": "Flux Shift",
        "ckpt_path_dir": "Checkpoint Path (Dir)",
        "llm_path": "LLM Path",
        "vae_path": "VAE Path",
        "clip_path": "CLIP Path",
        "save": "Save Model Config",
        "not_implemented": "Configuration fields for this model type explicitly not implemented in demo yet.",
        "v_pred": "V-Prediction Mode",
        "min_snr_gamma": "Min SNR Gamma",
        "debiased_estimation_loss": "Debiased Estimation Loss",
        "bypass_guidance_embedding": "Bypass Guidance Embedding",
        "single_file_path": "Single File Path",
        "first_frame_conditioning_p": "First Frame Cond Prob",
        "text_encoder_path": "Text Encoder Path",
        "t5_path": "T5 Path",
        "lumina_shift": "Lumina Shift",
        "min_t": "Min T",
        "max_t": "Max T",
        "llama3_path": "Llama3 Path",
        "llama3_4bit": "Llama3 4bit Quant",
        "max_llama3_sequence_length": "Max Llama3 Seq Length",
        "model_version": "Model Version",
        "shift": "Timestep Shift",
        "max_sequence_length": "Max Sequence Length",
        "byt5_path": "ByT5 Path",
        "diffusion_model": "Diffusion Model Path",
        "merge_adapters": "Merge Adapters",
        "diffusion_path": "Diffusion Path",
        "llm_adapter_lr": "LLM Adapter LR"
    },
    "model_load": {
        "title": "Model Loading Config",
        "desc": "Configure advanced model loading options like dtype and sampling method.",
        "dtype": "Base Dtype",
        "diffusion_transformer_dtype": "Transformer Dtype",
        "timestep_sample_method": "Timestep Sample Method",
        "model_type": "Model Load Type",
        "tooltip_dtype": "Base data type",
        "tooltip_transformer_dtype": "Transformer specific dtype (supports float8 for LoRA training)",
        "tooltip_sample_method": "Timestep sampling method, usually logit_normal",
        "tooltip_model_type": "Select model format type (diffusion / transformer)",
        "diffusion_model_dtype": "Diffusion Dtype",
        "transformer_dtype": "Transformer Dtype",
        "unet_dtype": "UNet Dtype",
        "model_dtype": "Model Dtype",
        "save": "Save Configuration",
        "auto_save_tooltip": "Parameters are auto-saved. Click to force save immediately."
    },
    "training": {
        "title": "Training Parameters",
        "desc": "Configure the training loop and optimization settings.",
        "output_name": "Output Name",
        "epochs": "Epochs",
        "lr_scheduler": "LR Scheduler",
        "batch_size": "Batch Size (per GPU)",
        "grad_accumulation": "Grad Accumulation",
        "warmup_steps": "Warmup Steps",
        "grad_clipping": "Grad Clipping",
        "save_dtype": "Save Dtype",
        "partition_method": "Partition Method",
        "activation_checkpointing": "Activation Checkpointing",
        "activation_checkpointing_unsloth": "Unsloth (Extreme VRAM Saving)",
        "pipeline_stages": "Pipeline Stages",
        "blocks_to_swap": "Blocks to Swap",
        "caching_batch_size": "Caching Batch Size",
        "video_clip_mode": "Video Clip Mode",
        "image_micro_batch_size_per_gpu": "Image Batch Size (Mixed)",
        "image_eval_micro_batch_size_per_gpu": "Image Eval Batch Size",
        "steps_per_print": "Steps Per Print",
        "save_every_n_epochs": "Save Every N Epochs",
        "checkpoint_every_n_minutes": "Checkpoint Every N Mins",
        "eval_settings": "Evaluation Settings",
        "validation_disabled_hint": "Model validation is disabled or no validation set configured. Settings hidden.",
        "eval_every_n_epochs": "Eval Every N Epochs",
        "eval_batch_size": "Eval Batch Size",
        "eval_grad_accumulation": "Eval Grad Accumulation",
        "eval_before_first_step": "Eval Before First Step",
        "disable_block_swap_for_eval": "Disable Block Swap for Eval",
        "start": "START TRAINING",
        "stop": "Stop Training",
        "training_stopped": "Training Stopped",
        "training_started": "Training Started",
        "failed_stop": "Failed to stop training",
        "launcher_desc": "Quick launch training with runtime overrides.",
        "config_summary": "Config Summary",
        "no_config_loaded": "No configuration loaded",
        "setup_hint": "Modify these in Training Setup tab.",
        "output_dir": "Output Directory"
    },
    "advanced_training": {
        "title": "Advanced Training Params",
        "desc": "Configure advanced training options like manual scheduler, memory optimizations, etc.",
        "max_steps": "Max Steps",
        "force_constant_lr": "Force Constant LR",
        "pseudo_huber_c": "Pseudo Huber C",
        "map_num_proc": "Data Processing Num Proc",
        "compile": "Compile Model (torch.compile)",
        "x_axis_examples": "Use Examples for X-Axis",
        "save_every_n_steps": "Save Every N Steps",
        "eval_every_n_steps": "Eval Every N Steps",
        "checkpoint_every_n_epochs": "Checkpoint Every N Epochs",
        "partition_split": "Manual Partition Split (comma separated)",
        "reentrant_activation_checkpointing": "Reentrant Activation Checkpointing"
    },
    "optimizer": {
        "title": "Optimizer Config",
        "desc": "Select optimizer type and adjust learning rate.",
        "type": "Optimizer Type",
        "lr": "Learning Rate",
        "beta1": "Beta 1",
        "beta2": "Beta 2",
        "weight_decay": "Weight Decay",
        "eps": "Epsilon",
        "stabilize": "Enable Training Stabilize"
    },
    "adapter": {
        "title": "Adapter Config",
        "desc": "Configure LoRA or other adapter settings. Selecting 'None' will enable Full Fine-Tuning (FFT).",
        "type": "Adapter Type",
        "rank": "Rank",
        "dtype": "Dtype",
        "init_from": "Init From Existing",
        "dropout": "Dropout",
        "fft_hint": "Tip: Full Fine-Tuning (FFT) mode is active. All model parameters will be trained; LoRA-specific settings are disabled."
    },
    "start_params": {
        "title": "Start Parameters",
        "desc": "Configure additional options for starting the training process.",
        "resume_from_checkpoint": "Resume From Checkpoint (Path or Empty)",
        "reset_dataloader": "Reset Dataloader (On Resume)",
        "regenerate_cache": "Force Regenerate Cache",
        "trust_cache": "Trust Existing Cache (Speedup)",
        "cache_only": "Cache Only & Exit (No Train)",
        "i_know_what_i_am_doing": "Skip Checks (Expert Only)",
        "dump_dataset": "Dump Dataset To (Debug Path)",
        "reset_optimizer_params": "Reset Optimizer Params (On Resume)",
        "num_gpus": "GPU Count"
    },
    "project": {
        "select_title": "Select Project",
        "select_desc": "Choose an existing training project or create a new one.",
        "new": "New Project",
        "open": "Open Project",
        "recent": "Recent Projects",
        "no_recent": "No recent projects found.",
        "last_modified": "Last modified",
        "path": "Path",
        "author_credit": "Created by TianDong",
        "new_desc": "Create a fresh training project in the output directory",
        "open_desc": "Open an existing project or drop a folder here",
        "rename_title": "Rename Project",
        "rename_confirm_desc": "Are you sure you want to rename this project to",
        "available_envs": "Detected Environments"
    },
    "monitoring": {
        "title": "Monitoring Config (WandB)",
        "desc": "Configure Weights & Biases monitoring",
        "enable_wandb": "Enable WandB Monitoring",
        "wandb_api_key": "WandB API Key",
        "wandb_tracker_name": "Tracker Name (Project)",
        "wandb_run_name": "Run Name"
    },
    "drop_zone": {
        "title": "Drop Project Folder Here",
        "desc": "Import trainconfig.toml, dataset.toml, and evaldataset.toml"
    },
    "validation": {
        "invalid_path": "Path cannot be empty",
        "invalid_resolutions": "Invalid resolutions format, e.g.: [512] or [[1280, 720]]，Remember that there is a space after the comma",
        "invalid_number": "Please enter a valid positive number",
        "invalid_integer": "Please enter an integer greater than 0",
        "invalid_json": "JSON parsing failed, please check brackets and commas",
        "min_max_ar": "Max AR must be greater than Min AR",
        "same_path_warning": "Warning: Training and Validation dataset paths are the same. Please ensure the validation set is independent.",
        "change_path": "Change Path"
    },
    "training_log": {
        "status_active": "Training in progress...",
        "status_inactive": "Training inactive. View last session logs.",
        "no_logs": "No logs recorded in this session.",
        "status": "Status",
        "empty": "No log output yet...",
        "current_session": "Current Live Session",
        "speed": "Training Speed",
        "iter_time": "Time per Step"
    },
    "help": {
        "title": "Parameter Details",
        "close": "Got it",
        "resolutions": "The resolution for training. Images will be scaled proportionally to these total pixel areas, including aspect ratio bucketing. You can enter a single value like [1024] or specific dimensions like [[1280, 720]].",
        "enable_ar_bucket": "Enables aspect ratio bucketing, allowing the model to learn from various image proportions without excessive cropping. Highly recommended for better composition capability.",
        "min_ar": "Minimum aspect ratio (width/height) allowed for bucketing. e.g., 0.5 means the narrowest is 1:2.",
        "max_ar": "Maximum aspect ratio (width/height) allowed for bucketing. e.g., 2.0 means the widest is 2:1.",
        "num_ar_buckets": "Number of AR buckets. More buckets mean more accurate matching but fewer samples per bucket. 3-5 is usually a good range. If your dataset contains many images with special aspect ratios, you can increase this value appropriately.",
        "num_repeats": "Number of times the dataset is repeated per Epoch. Increase to balance datasets of different sizes. If your images are few, you can increase this value appropriately.",
        "epochs": "Total number of training epochs. It is recommended to set a high value as you can stop training manually at any time.",
        "micro_batch_size_per_gpu": "The mini-batch size for a single forward/backward pass on the GPU. Increasing this speeds up training but consumes more VRAM.",
        "gradient_accumulation_steps": "Number of steps to accumulate gradients. This allows for larger effective batch sizes under limited VRAM. Effective Batch Size = Batch Size * Accumulation Steps * Num GPUs.",
        "warmup_steps": "Number of steps for learning rate warmup. Gradually increases the LR at the start to improve stability. Usually set to ~5% of total steps.",
        "lr_scheduler": "Type of learning rate scheduler. 'constant' keeps it fixed, 'linear' decreases it to 0 as training progresses.",
        "blocks_to_swap": "Offloads some model blocks to system memory to save VRAM. Higher values reduce VRAM usage but significantly slow down training, each model at least has 2 blocks in VRAM.",
        "activation_checkpointing": "Saves VRAM by recomputing intermediate activations. Recommended to keep enabled unless you have plenty of VRAM and need max speed,you can also use unsloth to save more VRAM.",
        "output_name": "The name of the folder where the trained models will be saved.",
        "grad_clipping": "Threshold for gradient clipping to prevent gradient explosion and improve stability.",
        "unet_lr": "Learning rate for the UNet module. Controls how fast the core model learns image features. Usually set slightly higher than text encoders.",
        "text_encoder_1_lr": "Learning rate for the first text encoder (CLIP-L). Controls how the model learns basic prompt concepts.",
        "text_encoder_2_lr": "Learning rate for the second text encoder (OpenCLIP-G). Affects understanding of more advanced language descriptions.",
        "min_snr_gamma": "Minimum SNR gamma value. Optimizes training by balancing loss across different noise levels. Recommended value is 5.0. Set to 0 to disable.",
        "v_pred": "Enable V-Prediction mode. A prediction target often used for 768p+ resolutions or specific fine-tuning goals, usually leading to more stable backgrounds.",
        "debiased_estimation_loss": "Enable debiased estimation loss. Balances training weights across noise levels to improve fine details in the generated images.",
        "flux_shift": "Whether to enable timestep shift for Flux models. Recommended for balanced efficiency and quality.",
        "bypass_guidance_embedding": "Bypass guidance embedding layer. Usually for specialized fine-tuning; recommended to keep disabled for standard use.",
        "lumina_shift": "Scheduler shift for Lumina models. Recommended to keep enabled as default.",
        "first_frame_conditioning_p": "Probability of using the first frame as conditioning in video training. Helps improve video consistency.",
        "min_t": "Minimum timestep. Controls the lower bound of noise levels the model focuses on during training.",
        "max_t": "Maximum timestep. Controls the upper bound of noise levels the model focuses on during training.",
        "llama3_4bit": "Whether to load the Llama3 model using 4-bit quantization, significantly reducing VRAM usage.",
        "max_llama3_sequence_length": "Maximum text sequence length processed by the Llama3 model. Higher values consume more VRAM.",
        "checkpoint_path": "Full path to the checkpoint file (.safetensors/.ckpt).",
        "diffusers_path": "Full path to the Diffusers-format model folder.",
        "transformer_path": "Path to the Transformer (U-Net) weights. Usually loaded from diffusers path if empty.",
        "vae_path": "Path to the VAE weights. Uses the default VAE if left blank.",
        "llm_path": "For LLM-based diffusion models (e.g., HunyuanVideo, Wan), this is the path to the LLM folder or weights.",
        "clip_path": "Path to the CLIP text encoder.",
        "t5_path": "Path to the T5 text encoder.",
        "single_file_path": "Full path to the single-file format weights.",
        "model_config_path": "Path to the model structure configuration file (e.g., JSON/Config). Usually optional.",
        "text_encoder_path": "Path to the text encoder weights.",
        "byt5_path": "Path to ByT5 text encoder weights, often used for models with glyph control (e.g., HunyuanImage 2.1).",
        "diffusion_model": "Path to the main diffusion model weight file.",
        "merge_adapters": "Path to the adapter weights to be merged (optional).",
        "max_sequence_length": "Maximum allowed text sequence length. Increasing this value consumes more VRAM but handles longer prompts.",
        "shift": "Timestep shift value. Controls noise scheduling; different models have different recommended values (e.g., HunyuanVideo 1.5 recommends 1.0, Flux 2 recommends 3.0).",
        "model_architecture": "Select the base model architecture to use. Different architectures are suitable for different tasks (e.g., SDXL/Flux for images, LTX/Hunyuan for videos).",
        "dataset_input_path": "Root directory path of the training dataset. This folder should contain images/videos and their corresponding caption files (.txt/.json/etc).",
        "control_path": "Folder path for ControlNet or other control condition weights. Usually used for training ControlNet or image/video editing tasks.",
        "eval_set_name": "Name of the validation set, used to identify different validation items in TensorBoard or logs.",
        "video_clip_mode": "Video sampling clipping mode: 'None' for random clipping; 'Single Beginning' for fixed sampling from the start; 'Single Middle' for sampling from the middle; 'Multiple Overlapping' splits one video into multiple overlapping segments for training.",
        "save_dtype": "The precision used when saving the trained model weights.",
        "partition_method": "Method to partition the model for multi-GPU pipeline parallelism. Usually keep at default.",
        "dtype": "Weight precision during training. bfloat16 is recommended for modern GPUs.",
        "transformer_dtype": "Weight precision for the Transformer part. Flux supported float8 to save massive VRAM.",
        "timestep_sample_method": "Method to sample timesteps during training. logit_normal is common to focus on image details.",
        "model_type": "Select the loading architecture for the model based on your files (single file vs folder).",
        "adapter_type": "Type of adapter. 'lora' is the standard choice. Delete or comment out the adapter section to perform Full Fine-Tuning (FFT).",
        "rank": "The rank of LoRA. Higher values mean more parameters and better expressive power but risk overfitting and higher VRAM usage.",
        "dropout": "The dropout probability for LoRA layers. Helps prevent overfitting by randomly 'dropping out' (setting to zero) a fraction of neurons during training. Recommended range: 0.0 to 0.1.",
        "init_from_existing": "You can initialize the adapter weights from a previously trained adapter (e.g., a LoRA). Enter the path to the saved weights folder.",
        "lr": "Learning Rate, controls the step size at each update. Too large can cause training to diverge, while too small will make training extremely slow. Fine-tune based on your model and optimizer.",
        "weight_decay": "A regularization technique to prevent overfitting.",
        "stabilize": "Enhances numerical stability during training through mathematical techniques, reducing the risk of gradient explosion.",
        "resume_from_checkpoint": "Continue training from a previous checkpoint. Enter the path to the checkpoint folder.",
        "dump_dataset": "Decodes the cached latents in the dataset and saves them as image files to the specified directory. Primarily used for debugging to verify that the cached data is correct.",
        "reset_dataloader": "When resuming, start the dataset from the beginning instead of where it left off.",
        "reset_optimizer_params": "When resuming, only load model weights and reset optimizer internal states (momentum, etc.).",
        "cache_only": "Only perform feature extraction and cache generation, then exit without training. Useful for pre-caching.",
        "regenerate_cache": "Force regeneration of dataset cache if you changed resolutions or images.",
        "trust_cache": "Skip re-validating images if you know they haven't changed, significantly speeds up setup.",
        "enable_wandb": "Enable online logging and monitoring via Weights & Biases.",
        "compile": "Compile the model using torch.compile (~20% speedup). Requires a few minutes of initialization at the start.several models do not support torch.compile, such as Zimage.",
        "map_num_proc": "Number of parallel processes for data preprocessing to speed up cache generation.",
        "image_micro_batch_size_per_gpu": "For mixed video/image training, you can specify a different batch size specifically for images. If not set, it defaults to the general micro_batch_size_per_gpu.",
        "pipeline_stages": "The number of pipeline parallelism stages to distribute the model across GPUs.",
        "caching_batch_size": "Batch size used for feature caching. Larger values speed up caching but use more VRAM.",
        "steps_per_print": "Number of steps between logging training progress.",
        "eval_every_n_epochs": "Frequency of evaluation in epochs.",
        "eval_every_n_steps": "Frequency of evaluation in steps. Set to 0 to disable step-based evaluation.",
        "eval_batch_size": "Batch size used during the evaluation phase.",
        "image_eval_micro_batch_size_per_gpu": "Batch size for images during the evaluation phase.",
        "eval_gradient_accumulation_steps": "Gradient accumulation steps for evaluation.",
        "eval_before_first_step": "Whether to perform an evaluation before the model starts training.",
        "disable_block_swap_for_eval": "Disable memory block swapping during evaluation to increase speed (requires enough VRAM).",
        "optimizer_type": "Type of optimizer. 'adamw_optimi' (default) uses Kahan summation for precision; 'adamw8bit' / 'kahan' variants save VRAM; 'offload' moves states to CPU. Any name from pytorch-optimizer is also supported.",
        "beta1": "Beta 1 parameter for the AdamW optimizer, typically 0.9.",
        "beta2": "Beta 2 parameter for the AdamW optimizer, typically 0.99 or 0.999.",
        "eps": "Term added to the denominator to improve numerical stability (Epsilon).prevent model from oscillation",
        "max_steps": "Maximum number of training steps. Training stops once reached.",
        "force_constant_lr": "Enforce a constant learning rate, overriding any scheduler settings.",
        "pseudo_huber_c": "Controls the sensitivity of the loss function to errors, used to balance detail capture and training stability. Recommended to keep at default unless you notice unusual noise handling.",
        "save_every_n_steps": "Frequency of saving model weights or checkpoints in steps.",
        "checkpoint_every_n_epochs": "Frequency of saving full training state (checkpoint) in epochs.",
        "partition_split": "Model partitioning strategy for distributing complex models across GPUs.",
        "x_axis_examples": "Number of examples to display on the X-axis during visualization.",
        "reentrant_activation_checkpointing": "Use reentrant activation checkpointing. Can save more VRAM in some architectures but might impact gradient stability.",
        "ar_buckets": "Custom aspect ratio buckets. Usually a list of resolution pairs to control image bucketing.",
        "frame_buckets": "Custom frame buckets. Usually a list of frame counts to control video length bucketing, for example, if the video frame count is 48, it will be placed in the 32 frame bucket, not the 64 frame bucket.",
        "save_every_n_epochs": "Frequency of saving full training state (checkpoint) in epochs.",
        "eval_dataset": "The evaluation dataset is used to assess the model's performance during training. By testing on data independent of the training set(Note: the evaluation dataset cannot be the same as the training dataset, you can also split a part of the training dataset as the evaluation dataset, but it is not recommended to use the same images), you can monitor for overfitting and track the actual progress of generation quality.",
        "num_gpus": "Specify the number of GPUs to use for training.",
        "checkpoint_every_n_minutes": "Frequency of saving full training state (checkpoint) in minutes, which can let you resume training from the checkpoint if the training is interrupted.",
        "llm_adapter_lr": "Learning rate for the LLM adapter. This is an adapter that processes LLM embeddings before feeding them into the diffusion model. Setting this to 0 disables its training,which may makes training more stable for small datasets, if you have a large dataset or many new concepts, you can try training llm_adapter to see if it helps"
    },
    "diagnostics": {
        "title": "System Diagnostics",
        "description": "Check Python environment integrity and system status",
        "env_status": "Training core",
        "python_path": "Python Path",
        "total_files": "Total Files",
        "total_size": "Total Size",
        "fingerprint": "Fingerprint",
        "calculate": "Calculate Fingerprint",
        "calculating": "Calculating...",
        "official_fingerprint": "Official Fingerprint",
        "match": "✓ Integrity Check Passed",
        "mismatch": "✗ Integrity Check Failed",
        "no_official": "Official fingerprint file not found",
        "diff_files": "Changed Files",
        "missing": "Missing",
        "changed": "Changed",
        "added": "Added",
        "version_info": "Version Info",
        "app_version": "App Version",
        "click_to_view": "Click to view full hash",
        "local_hash": "Local Fingerprint Detail",
        "official_hash": "Official Fingerprint Detail",
        "hash_type": "SHA256 Fingerprint",
        "last_calculated": "Last calculated: {{date}}",
        "local": "Local",
        "official": "Official",
        "more_diff_files": "... and {{count}} more different files"
    }
}