import argparse
import os
import json
import torch
import torch.nn as nn
from collections import defaultdict
from pathlib import Path
from safetensors.torch import load_file
import glob
import gc

# å°è¯•å¯¼å…¥ rich
try:
    from rich.console import Console
    from rich.table import Table
    from rich.tree import Tree
    from rich import print as rprint
    HAS_RICH = True
    console = Console()
except ImportError:
    HAS_RICH = False
    print("æç¤º: å»ºè®®å®‰è£… 'rich' åº“ä»¥è·å¾—æœ€ä½³æ˜¾ç¤ºæ•ˆæœ: pip install rich")

def format_size(num_params):
    if num_params >= 1e9: return f"{num_params / 1e9:.2f}B"
    elif num_params >= 1e6: return f"{num_params / 1e6:.2f}M"
    elif num_params >= 1e3: return f"{num_params / 1e3:.2f}K"
    return str(num_params)

class ModelAnalyzer:
    def __init__(self, show_all_keys=False):
        self.show_all_keys = show_all_keys
        # å…¨å±€ç»Ÿè®¡å®¹å™¨
        self.global_tree = defaultdict(lambda: {"params": 0})
        self.global_total_params = 0
        self.global_keys = []
        self.scanned_files = []

    def analyze_diffusers(self, model_path):
        print(f"\n======== åˆ†æ Diffusers æ¨¡å‹: {model_path} ========\n")
        
        # 1. æ‰“å° Config (å¦‚æœæœ‰)
        self.print_config(model_path)
        
        # 2. å°è¯•æ ‡å‡†åŠ è½½ (ä¸ºäº†å…¼å®¹éè‡ªå®šä¹‰æ¨¡å‹)
        try:
            from diffusers import UNet2DConditionModel, Transformer2DModel
            import logging
            # æŠ‘åˆ¶ diffusers çš„æŠ¥é”™æ—¥å¿—
            logging.getLogger("diffusers").setLevel(logging.ERROR)
            
            # å°è¯• Transformer
            if os.path.exists(os.path.join(model_path, "transformer")):
                try:
                    # ä»…ä½œå°è¯•ï¼Œä¸å¼ºæ±‚æˆåŠŸ
                    model = Transformer2DModel.from_pretrained(model_path, subfolder="transformer")
                    print("âœ… æˆåŠŸé€šè¿‡ Diffusers åŠ è½½æ¶æ„ (Transformer2DModel)")
                    self.analyze_live_model(model)
                    return
                except: pass
        except ImportError:
            pass

        print("âš ï¸ æ ‡å‡†åŠ è½½å¤±è´¥æˆ–è·³è¿‡ (æ£€æµ‹ä¸ºè‡ªå®šä¹‰æ¶æ„/åˆ†å·æƒé‡)ã€‚")
        print("ğŸ”„ åˆ‡æ¢åˆ° [åˆ†å·åˆå¹¶æ‰«ææ¨¡å¼] ...")
        self.scan_sharded_weights(model_path)

    def scan_sharded_weights(self, model_path):
        # æœç´¢é€»è¾‘ï¼šä¼˜å…ˆæ‰¾ transformer æ–‡ä»¶å¤¹ï¼Œå…¶æ¬¡ unetï¼Œæœ€åæ ¹ç›®å½•
        search_paths = [
            os.path.join(model_path, "transformer"),
            os.path.join(model_path, "unet"),
            model_path
        ]
        
        target_files = []
        
        # 1. å®šä½åŒ…å«æƒé‡çš„æ–‡ä»¶å¤¹
        for p in search_paths:
            if not os.path.exists(p): continue
            
            # æŸ¥æ‰¾ safetensors
            files = glob.glob(os.path.join(p, "*.safetensors"))
            # è¿‡æ»¤æ‰ optimizer æˆ– text_encoder (é€šå¸¸æˆ‘ä»¬åªå…³å¿ƒä¸»æ¨¡å‹)
            files = [f for f in files if "optimizer" not in f and "text_encoder" not in f]
            
            if files:
                target_files = files
                print(f"ğŸ“‚ åœ¨æ–‡ä»¶å¤¹å‘ç°æƒé‡: {p}")
                break
        
        if not target_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½• .safetensors æƒé‡æ–‡ä»¶ã€‚")
            return

        target_files.sort()
        print(f"ğŸ“¦ æ£€æµ‹åˆ° {len(target_files)} ä¸ªæƒé‡åˆ†ç‰‡ï¼Œå¼€å§‹é€ä¸ªåˆ†æ...")
        
        # 2. é€ä¸ªæ–‡ä»¶è¯»å–å¹¶åˆå¹¶ä¿¡æ¯
        for i, file_path in enumerate(target_files):
            file_name = os.path.basename(file_path)
            print(f"   [{i+1}/{len(target_files)}] è¯»å–: {file_name} ...", end="\r")
            try:
                self.process_single_file_content(file_path)
            except Exception as e:
                print(f"\n   âŒ è¯»å–å¤±è´¥ {file_name}: {e}")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé˜²æ­¢å†…å­˜çˆ†æ‰
            gc.collect()

        print(f"\nâœ… æ‰«æå®Œæˆï¼å…±åˆ†æ {len(self.scanned_files)} ä¸ªæ–‡ä»¶ã€‚\n")
        self.print_final_report()

    def process_single_file_content(self, file_path):
        """è¯»å–å•ä¸ªæ–‡ä»¶å¹¶ç´¯åŠ åˆ°å…¨å±€ç»Ÿè®¡ä¸­"""
        state_dict = load_file(file_path)
        self.scanned_files.append(os.path.basename(file_path))
        
        for key, tensor in state_dict.items():
            # è®°å½• Key å’Œ Shape ç”¨äºå±•ç¤º
            shape_str = str(list(tensor.shape))
            self.global_keys.append((key, shape_str))
            
            # ç»Ÿè®¡å‚æ•°é‡
            n = tensor.numel()
            self.global_total_params += n
            
            # æ„å»ºå±‚çº§æ ‘ (å–å‰ä¸¤çº§ä½œä¸º Key)
            parts = key.split('.')
            if len(parts) >= 2:
                prefix = f"{parts[0]}.{parts[1]}"
            else:
                prefix = parts[0]
            
            self.global_tree[prefix]["params"] += n
        
        del state_dict # ç«‹å³é‡Šæ”¾å†…å­˜

    def print_final_report(self):
        # 1. æ‰“å° Keys (å¦‚æœå¼€å¯)
        if self.show_all_keys:
            print(f"\n======== å®Œæ•´ Module/Key åˆ—è¡¨ ({len(self.global_keys)} ä¸ª) ========\n")
            # æ’åº
            self.global_keys.sort(key=lambda x: x[0])
            for key, shape in self.global_keys:
                print(f"{key:<70} | {shape}")
            print("\n" + "="*50)
        else:
            print(f"(æç¤º: ä½¿ç”¨ --show_keys å¯æŸ¥çœ‹ {len(self.global_keys)} ä¸ª Key çš„å®Œæ•´åç§°åˆ—è¡¨)")

        # 2. æ€»å‚æ•°
        print(f"\nğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {format_size(self.global_total_params)}")
        
        # 3. æ¨¡å—åˆ†å¸ƒ
        print("\n-------- æ¨¡å—å‚æ•°åˆ†å¸ƒ (Top 2000) --------")
        sorted_tree = sorted(self.global_tree.items(), key=lambda x: x[1]['params'], reverse=True)
        
        if HAS_RICH:
            table = Table(title=f"Block Analysis (Total: {format_size(self.global_total_params)})")
            table.add_column("Block Name", style="cyan")
            table.add_column("Params", style="magenta")
            table.add_column("Ratio", style="yellow")
            
            for k, v in sorted_tree[:2000]: # åªçœ‹å‰20ä¸ªå¤§å—
                p = v['params']
                ratio = (p / self.global_total_params) * 100
                table.add_row(k, format_size(p), f"{ratio:.1f}%")
            console.print(table)
        else:
            for k, v in sorted_tree[:20]:
                p = v['params']
                ratio = (p / self.global_total_params) * 100
                print(f"{k:<50} : {format_size(p)} ({ratio:.1f}%)")

    def print_config(self, model_path):
        # ç®€æ˜“ Config è¯»å–
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path):
             config_path = os.path.join(model_path, "transformer", "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                conf = json.load(f)
            print("\n--- é…ç½®æ‘˜è¦ ---")
            keys = ["_class_name", "architectures", "num_attention_heads", "attention_head_dim", "in_channels", "patch_size", "num_layers"]
            for k in keys:
                if k in conf: print(f"{k}: {conf[k]}")

    def analyze_live_model(self, model):
        # å…¼å®¹æ—§é€»è¾‘ï¼šå¦‚æœæ ‡å‡†åŠ è½½æˆåŠŸï¼Œç›´æ¥åˆ†æ model å¯¹è±¡
        total = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {format_size(total)}")
        
        if self.show_all_keys:
            print("\n======== Module åˆ—è¡¨ ========\n")
            for name, mod in model.named_modules():
                # åªæ‰“å°å¶å­èŠ‚ç‚¹
                if len(list(mod.children())) == 0 and sum(p.numel() for p in mod.parameters()) > 0:
                     shapes = [str(list(p.shape)) for p in mod.parameters(recurse=False)]
                     print(f"{name:<60} | {', '.join(shapes)}")
                     
        # ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŠŠ module è½¬æ¢æˆ tree æ¥å¤ç”¨åˆ†å¸ƒæ‰“å°é€»è¾‘ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ç›´æ¥æ‰“å°
        print("\n(æ ‡å‡†åŠ è½½æ¨¡å¼ä¸‹ï¼Œè¯¦ç»†åˆ†å¸ƒå»ºè®®å‚è€ƒ named_modules åˆ—è¡¨)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--path", type=str, required=True)
    parser.add_argument("--show_keys", action="store_true")
    args = parser.parse_args()

    analyzer = ModelAnalyzer(show_all_keys=args.show_keys)
    
    if os.path.isfile(args.path):
        print("æ£€æµ‹åˆ°å•æ–‡ä»¶ï¼Œåˆ†æä¸­...")
        analyzer.process_single_file_content(args.path)
        analyzer.print_final_report()
    elif os.path.isdir(args.path):
        analyzer.analyze_diffusers(args.path)
    else:
        print(f"è·¯å¾„ä¸å­˜åœ¨: {args.path}")